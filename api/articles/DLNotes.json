{"title":"DeepLearning-Note","uid":"bf5e45ddd75ae274164ba73bf6dc58dd","slug":"DLNotes","date":"2024-11-06T09:16:39.000Z","updated":"2024-12-29T14:18:40.203Z","comments":true,"path":"api/articles/DLNotes.json","keywords":null,"cover":"https://zh.d2l.ai/_images/front.png","content":"<h1 id=\"Chapter1-amp-2-Introduction-amp-Pre-Knowledge\"><a href=\"#Chapter1-amp-2-Introduction-amp-Pre-Knowledge\" class=\"headerlink\" title=\"Chapter1&amp;2  Introduction&amp;Pre-Knowledge\"></a>Chapter1&amp;2  Introduction&amp;Pre-Knowledge</h1><p>张量的概念——由数值组成的数组，可以有多个维度。</p>\n<ol>\n<li>torch.arange(x)–生成一个0-x的一维张量，左开右闭。</li>\n<li>reshape和shape和numpy中的一样。</li>\n<li><strong>torch.numel()返回张量中元素的数量，始终为标量。</strong></li>\n<li>torch.tensor(List)方法可以使得直接用列表赋值，产生一个对应的张量。</li>\n<li>张量可以作＋－×&#x2F; ** 求幂 这些运算，运算均对每一个元素进行。</li>\n<li>torch.cat((X, Y), dim&#x3D;0) 通过cat方法可以将两个张量拼接起来，dim指定合并在第几维，比如dim&#x3D;0，在第0维合并，也就是按行合并，dim&#x3D;1，也就是按列合并，以此类推。</li>\n<li>X.sum()求和方法。返回一个只有一个元素的<strong>张量</strong></li>\n<li>torch.tensor.item()方法可以取出张量中的元素，仅限0维的张量，只有一个元素的那种。</li>\n</ol>\n<h2 id=\"广播机制\"><a href=\"#广播机制\" class=\"headerlink\" title=\"广播机制\"></a>广播机制</h2><p>对于形状不同的两个张量，基于广播机制，我们仍然可以进行按元素操作，比如相加相乘。比如：</p>\n<pre class=\"line-numbers language-Python\" data-language=\"Python\"><code class=\"language-Python\">tensor([[0],\n        [1],\n        [2]])\ntensor([[0, 1]])\ntensor([[0, 1],\n        [1, 2],\n        [2, 3]])\n\n# 对于第一第二个张量，两个相加之后会产生如上所示的第三个张量，也就是说，形状不一样的张量，他会自动扩展成一样的，然后进行计算。\n0     0 0\n1  -&gt; 1 1\n2     2 2\n0 1 -&gt;  0 1\n        0 1\n        0 1\n如上变换之后，再进行按位计算。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h2 id=\"处理缺失的数据\"><a href=\"#处理缺失的数据\" class=\"headerlink\" title=\"处理缺失的数据\"></a>处理缺失的数据</h2><p>常用的方法有插值和删除。插值就是将NaN的数据修改成一个正常的数据，删除就是将含有NaN的那行数据直接删除。</p>\n<p>例如一下的插值方法：</p>\n<pre class=\"line-numbers language-Python\" data-language=\"Python\"><code class=\"language-Python\">inputs, outputs &#x3D; data.iloc[:, 0:2], data.iloc[:, 2]\ninputs &#x3D; inputs.fillna(inputs.mean())\nprint(inputs)\n\n   NumRooms Alley\n0       3.0  Pave\n1       2.0   NaN\n2       4.0   NaN\n3       3.0   NaN\n\n结果上，将NumRooms中的NaN替换成了现有数据的平均值。\niloc为indexloction根据下标来获取子数组\nfillna用于填充NaN的数据。\ninputs.mean()用于计算每一类别的平均值，如果可以计算的话。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>对于数组中的类别值或者离散值，我们将NaN看成一个类别。比如：</p>\n<pre class=\"line-numbers language-Python\" data-language=\"Python\"><code class=\"language-Python\">inputs &#x3D; pd.get_dummies(inputs, dummy_na &#x3D; True)\n通过这个方法之后，inputs变为了如下\nNumRooms  Alley_Pave  Alley_nan\n0       3.0           1          0\n1       2.0           0          1\n2       4.0           0          1\n3       3.0           0          1\n\n相当于将原来Alley属性中的所有离散值拆成了表格中的每一个属性，然后通过0和1来表示这条记录中的Alley是什么。这样就消除了NaN。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>综上，进行了一个数据的预处理，将csv中的数据处理掉NaN的值，最终转化为torch中使用的张量。</p>\n<h2 id=\"数学基础\"><a href=\"#数学基础\" class=\"headerlink\" title=\"数学基础\"></a>数学基础</h2><h3 id=\"范数\"><a href=\"#范数\" class=\"headerlink\" title=\"范数\"></a>范数</h3><p>$c &#x3D; A*b  \\ hence \\  ||c|| \\le ||A|| * ||b||$</p>\n<p>取决于如何衡量b和c的长度，b和c都是向量。</p>\n<p>常见范数：</p>\n<p>矩阵范数：最小的满足上述公式的值</p>\n<p><strong>Frobenius范数：$||A||<em>{Frob} &#x3D; [\\sum</em>{ij} A_{ij}^2]^{\\frac{1}{2}}$</strong></p>\n<h3 id=\"正交矩阵\"><a href=\"#正交矩阵\" class=\"headerlink\" title=\"正交矩阵\"></a>正交矩阵</h3><ol>\n<li>所有行都相互正交</li>\n<li>所有行都有单位长度</li>\n<li>可以写成$U*U^T&#x3D;1$</li>\n</ol>\n<h3 id=\"置换矩阵\"><a href=\"#置换矩阵\" class=\"headerlink\" title=\"置换矩阵\"></a>置换矩阵</h3><p>置换矩阵是一个方形<a href=\"https://baike.baidu.com/item/%E4%BA%8C%E8%BF%9B%E5%88%B6/361457?fromModule=lemma_inlink\">二进制</a>矩阵，它在每行和每列中只有一个1，而在其他地方则为0。</p>\n<p>设P 是一个 m×n 的 (0,1) 矩阵，如果 m≤n且 PP′&#x3D;E，则称 P为一个 m×n的置换矩阵。其中P′是P的<a href=\"https://baike.baidu.com/item/%E8%BD%AC%E7%BD%AE%E7%9F%A9%E9%98%B5/3380917?fromModule=lemma_inlink\">转置矩阵</a>，E是m阶单位方阵。</p>\n<h3 id=\"哈达玛积-Hadamard-product\"><a href=\"#哈达玛积-Hadamard-product\" class=\"headerlink\" title=\"哈达玛积(Hadamard product)\"></a>哈达玛积(Hadamard product)</h3><p>两个矩阵的按元素乘法称为哈达玛积。数学符号为$\\odot$</p>\n<h3 id=\"L-2-范数是向量元素平方和的平方根\"><a href=\"#L-2-范数是向量元素平方和的平方根\" class=\"headerlink\" title=\"$L_2$范数是向量元素平方和的平方根\"></a>$L_2$范数是向量元素平方和的平方根</h3><p>$||x||<em>2 &#x3D; \\sqrt{\\sum</em>{i&#x3D;1}^{n}x_i^2}$</p>\n<h3 id=\"L-1-范数是向量元素的绝对值之和\"><a href=\"#L-1-范数是向量元素的绝对值之和\" class=\"headerlink\" title=\"$L_1$范数是向量元素的绝对值之和\"></a>$L_1$范数是向量元素的绝对值之和</h3><p>$||x||<em>1&#x3D;\\sum</em>{i&#x3D;1}^{n}|x_i|$</p>\n<h3 id=\"亚导数\"><a href=\"#亚导数\" class=\"headerlink\" title=\"亚导数\"></a>亚导数</h3><p>将导数拓展到不可微的函数</p>\n<p>$$\\frac{\\partial |x|}{\\partial x}&#x3D;\\begin{cases}1 , &amp; x &gt; 0 \\ -1,  &amp;x&lt;0 \\ a , &amp; x&#x3D;0, a \\in [-1,1] \\end{cases}$$</p>\n<p>其中a为该区间内的任意数。</p>\n<h3 id=\"矩阵求导公式\"><a href=\"#矩阵求导公式\" class=\"headerlink\" title=\"矩阵求导公式\"></a>矩阵求导公式</h3><p><img src=\"/../photos/1.png\" alt=\"1\"></p>\n<p><img src=\"/../photos/2.png\" alt=\"2\"></p>\n<h3 id=\"自动求导\"><a href=\"#自动求导\" class=\"headerlink\" title=\"自动求导\"></a>自动求导</h3><p>自动求导是计算一个函数在指定值上的导数</p>\n<p>有别于：</p>\n<ol>\n<li>符号求导，给你一个f(x)，求f’(x)</li>\n<li>数值求导：用一个逼近值去拟合在某一点的导数值。</li>\n</ol>\n<h3 id=\"自动求导实现\"><a href=\"#自动求导实现\" class=\"headerlink\" title=\"自动求导实现\"></a>自动求导实现</h3><p>涉及向量梯度的求解，实现方法如下</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 或者先构造x，然后调用x.requires_grad_(True)</span>\n<span class=\"token comment\"># 这样之后，y对于x的梯度会存储在x.grad里面</span>\ny <span class=\"token operator\">=</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> torch<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>\ny<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>\n<span class=\"token comment\">#然后通过调用y的反向传播函数来进行求导，得到的结果通过x.grad访问</span>\n通过x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">.</span>zero_<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>将梯度清空，因为pytorch会默认累计梯度值<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h1 id=\"Chapter-3-Linear-Neural-Network\"><a href=\"#Chapter-3-Linear-Neural-Network\" class=\"headerlink\" title=\"Chapter 3 Linear Neural Network\"></a>Chapter 3 Linear Neural Network</h1><h2 id=\"Linear-regression\"><a href=\"#Linear-regression\" class=\"headerlink\" title=\"Linear regression\"></a>Linear regression</h2><p>衡量预估质量：</p>\n<p>​\t平方损失：$y&#x3D;\\frac{(y-y’)^2}{2}$其中y’为估计值，y为真实值。</p>\n<h3 id=\"调用现有框架来进行线性回归的使用\"><a href=\"#调用现有框架来进行线性回归的使用\" class=\"headerlink\" title=\"调用现有框架来进行线性回归的使用\"></a>调用现有框架来进行线性回归的使用</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># 使用nn的预定好的层，线性回归实际就是线性的神经网络</span>\n<span class=\"token keyword\">from</span> torch <span class=\"token keyword\">import</span> nn\n\nnet <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Linear中规定输入的维度和输出的维度。</span>\n\n<span class=\"token comment\"># 初始化模型参数</span>\nnet<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>normal_<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span>\nnet<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>bias<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>fill_<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># weight 就是w， bias就是b</span>\n\n<span class=\"token comment\"># 计算均方误差，使用MSEloss类</span>\nloss <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 实例化梯度下降算法，SGD</span>\ntrainer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr <span class=\"token operator\">=</span> <span class=\"token number\">0.03</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 需要所使用的network中的所有参数，以及学习率</span>\n\n\n<span class=\"token comment\"># 构造真实的w和b</span>\ntrue_w <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">3.4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\ntrue_b <span class=\"token operator\">=</span> <span class=\"token number\">4.2</span>\nfeatures<span class=\"token punctuation\">,</span> labels <span class=\"token operator\">=</span> d2l<span class=\"token punctuation\">.</span>generate_data<span class=\"token punctuation\">(</span>true_w<span class=\"token punctuation\">,</span> true_b<span class=\"token punctuation\">,</span> num_examples<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 这部分都是自己写的，和库无关。</span>\n\n<span class=\"token comment\"># 读取数据集</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">load_array</span><span class=\"token punctuation\">(</span>data_arrays<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> is_train<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>  <span class=\"token comment\">#@save</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"构造一个PyTorch数据迭代器\"\"\"</span>\n    dataset <span class=\"token operator\">=</span> data<span class=\"token punctuation\">.</span>TensorDataset<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>data_arrays<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> data<span class=\"token punctuation\">.</span>DataLoader<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> shuffle<span class=\"token operator\">=</span>is_train<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 之后就可以通过load方法来获取数据迭代器，来迭代获取各batch数据</span>\nbatch_size <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\ndata_iter <span class=\"token operator\">=</span> load_array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>features<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">)</span>\n\n\n\n<span class=\"token comment\"># 训练过程</span>\nnum_epoch <span class=\"token operator\">=</span> <span class=\"token number\">3</span>\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_epoch<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">for</span> X<span class=\"token punctuation\">,</span> y <span class=\"token keyword\">in</span> data_iter<span class=\"token punctuation\">:</span> <span class=\"token comment\"># 遍历每一个batch数据</span>\n    l  <span class=\"token operator\">=</span> loss<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 计算对应的损失</span>\n    trainer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># grad清零</span>\n    l<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token comment\"># 进行反向传播</span>\n    trainer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 更新w和b参数</span>\n  l <span class=\"token operator\">=</span> loss<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">(</span>features<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>message<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p>L2 loss</p>\n<p>$l(y, y’)&#x3D;\\frac{(y-y’)^2}{2}$</p>\n<p>L1 loss</p>\n<p>$l(y,y’)&#x3D;|y-y’|$</p>\n<p>Huber’s Robust Loss</p>\n<p>$l(y,y’)&#x3D;\\begin{cases} |y-y’|-\\frac{1}{2} \\ if \\ |y-y’|&gt;1 \\ \\frac{(y-y’)^2}{2} \\ otherwise \\end{cases}$</p>\n<h2 id=\"softmax回归\"><a href=\"#softmax回归\" class=\"headerlink\" title=\"softmax回归\"></a>softmax回归</h2><p>softmax回归其实是一种分类模型，他是将输入根据预定好的n个参数，计算出来预定好的k个分类的置信度。输出的每个分类都有一个置信度，也就是一个概率，概率最高的那个，我们就认为模型预测的就是那个。</p>\n<p>而对于label，就相当于某个分类的置信度是1，其他都是0，如此我们可以将输出量化。</p>\n<p>softmax具有一个softmax(x)的子操作，我们对于一个输入的example，通过softmax计算出k中分类他们对应的置信度，得到的一个向量。</p>\n<p>对于损失函数，我们使用预测值和真实值的交叉熵作为损失函数的值，用来进行模型参数的优化。</p>\n<h3 id=\"pytorch实现\"><a href=\"#pytorch实现\" class=\"headerlink\" title=\"pytorch实现\"></a>pytorch实现</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># 设置batchsize大小，获取训练数据和测试数据</span>\n \tbatch_size <span class=\"token operator\">=</span> <span class=\"token number\">256</span>\n   train_iter<span class=\"token punctuation\">,</span> test_iter <span class=\"token operator\">=</span> load_data_fashion_mnist<span class=\"token punctuation\">(</span>batch_size<span class=\"token operator\">=</span>batch_size<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 在神经网络中开辟线性层，同时因为线性网络是全连接层，所以必须要将数据展开</span>\n   net <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">784</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 初始化weight，一层一层的初始化</span>\n   <span class=\"token keyword\">def</span> <span class=\"token function\">init_weight</span><span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n       <span class=\"token keyword\">if</span> <span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">:</span>\n           nn<span class=\"token punctuation\">.</span>init<span class=\"token punctuation\">.</span>normal_<span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">,</span> mean <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> std <span class=\"token operator\">=</span> <span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span>\n\n   net<span class=\"token punctuation\">.</span><span class=\"token builtin\">apply</span><span class=\"token punctuation\">(</span>init_weight<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 定义损失函数，使用交叉熵</span>\n   loss <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 定义训练方法，使用SGD方法</span>\n   trainer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr <span class=\"token operator\">=</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 训练过程，和线性回归类似</span>\n   num_epochs <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n   <span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_epochs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n       <span class=\"token keyword\">for</span> X<span class=\"token punctuation\">,</span> y <span class=\"token keyword\">in</span> train_iter<span class=\"token punctuation\">:</span>\n           l <span class=\"token operator\">=</span> loss<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n           trainer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n           l<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n           trainer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n       loss_list <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n       <span class=\"token keyword\">for</span> X<span class=\"token punctuation\">,</span> y <span class=\"token keyword\">in</span> test_iter<span class=\"token punctuation\">:</span>\n           <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n               l <span class=\"token operator\">=</span> loss<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n               loss_list<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>l<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n       tensorloss <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span>loss_list<span class=\"token punctuation\">)</span>\n       avg_loss <span class=\"token operator\">=</span> tensorloss<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n       <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"epoch: </span><span class=\"token interpolation\"><span class=\"token punctuation\">&#123;</span>epoch <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">&#125;</span></span><span class=\"token string\">, loss: </span><span class=\"token interpolation\"><span class=\"token punctuation\">&#123;</span>avg_loss<span class=\"token punctuation\">&#125;</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h3 id=\"计算准确率和准确的个数\"><a href=\"#计算准确率和准确的个数\" class=\"headerlink\" title=\"计算准确率和准确的个数\"></a>计算准确率和准确的个数</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># 单次预测上的准确个数</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">accurate_numbers</span><span class=\"token punctuation\">(</span>y_hat<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y_hat<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span> <span class=\"token keyword\">and</span> y_hat<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n        y_hat <span class=\"token operator\">=</span> y_hat<span class=\"token punctuation\">.</span>argmax<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    <span class=\"token builtin\">cmp</span> <span class=\"token operator\">=</span> y_hat<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> y\n    <span class=\"token keyword\">return</span> <span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">cmp</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 在数据集上的正确率</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">accurate_rate</span><span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">,</span> data_iter<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># 如果net是nn'里面的，net需要修改成评估模式，禁止梯度计算</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        net<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    number_accurate <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span>\n    number_all <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span>\n    <span class=\"token keyword\">for</span> X<span class=\"token punctuation\">,</span> y <span class=\"token keyword\">in</span> data_iter<span class=\"token punctuation\">:</span>\n        number_accurate <span class=\"token operator\">+=</span> accurate_numbers<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n        number_all <span class=\"token operator\">+=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> number_accurate <span class=\"token operator\">/</span> number_all<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>其实就是比较y_hat和y在类型上有多少相同的。</p>\n<h1 id=\"Chapter-4-感知机\"><a href=\"#Chapter-4-感知机\" class=\"headerlink\" title=\"Chapter 4 感知机\"></a>Chapter 4 感知机</h1><h2 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h2><p>给定输入x， 权重w，偏移b，感知机的输出为：</p>\n<p>$o &#x3D; \\sigma (&lt;w, x&gt; + b) \\ \\ \\ \\ \\ \\sigma (x) &#x3D; \\begin{cases} 1 \\ if \\ x &gt; 0 \\ -1 \\ otherwise \\end{cases}$</p>\n<p>他是一种二分类。</p>\n<p>训练过程如下：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">w&#x3D;0, b&#x3D;0\nrepeat \n\tif yi[&lt;w,xi&gt;+b]&lt;&#x3D;0 then\n\tw &#x3D; w + yi*xi \n\tb &#x3D; b + yi\nend if\nuntil all classified correctly<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>训练过程等价于batchsize&#x3D;1的梯度下降，使用的loss函数如下：</p>\n<p>$l(y,x,w) &#x3D; max(0, -y&lt;w,x&gt;)$</p>\n<h3 id=\"感知机收敛定理\"><a href=\"#感知机收敛定理\" class=\"headerlink\" title=\"感知机收敛定理\"></a>感知机收敛定理</h3><p>requirements：</p>\n<ol>\n<li>数据范围在半径r的范围之内</li>\n<li>存在一个余量$\\sigma$ 使得两种分类之间有至少一个余量的间隔——也就是分类数据之间不能贴太紧</li>\n</ol>\n<p>定理：综上，可得，感知机保证在$\\frac{r^2 + 1}{\\rho ^2}$ 步之后收敛</p>\n<h3 id=\"感知机的局限性\"><a href=\"#感知机的局限性\" class=\"headerlink\" title=\"感知机的局限性\"></a>感知机的局限性</h3><p>感知机只能产生线性分割面，不能处理XOR问题。</p>\n<h2 id=\"多层感知机-MLP\"><a href=\"#多层感知机-MLP\" class=\"headerlink\" title=\"多层感知机(MLP)\"></a>多层感知机(MLP)</h2><p>因为感知机存在局限性，只能产生线性分割面，所以产生了多层感知机</p>\n<p>多层感知机相较于单层感知机，其实就是添加了一个中间层，其中中间层的输出等于上一层的输入经过一个非线性公式变换。注意，一定是非线性变换，不然最终的多层感知机其实还是等价于一个单层感知机。这个非线性变换函数称为<strong>激活函数</strong> </p>\n<p>常见激活函数：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Sigmoid函数：$sigmoid(x) &#x3D; \\frac{1}{1+e^{-x}}$</p>\n<p>tanh函数：$tanh(x) &#x3D; \\frac{1-e^{-2x}}{1+e^{-2x}}$</p>\n<p>ReLU(rectified linear unit)函数：$ReLU(x) &#x3D; max(x, 0)$</p></blockquote>\n<p>多层感知机可以设置多层隐藏层，同时也可以调整每个隐藏层的大小。</p>\n<p><strong>超参数：隐藏层数，每层隐藏层的大小</strong></p>\n<h2 id=\"权重衰退-weight-decay\"><a href=\"#权重衰退-weight-decay\" class=\"headerlink\" title=\"权重衰退(weight decay)\"></a>权重衰退(weight decay)</h2><p>一种控制过拟合的方法。</p>\n<p>使用均方范数作为硬性条件：</p>\n<p>$min \\  l(w, b) \\ \\ \\ subject \\  to \\ ||w||^2 \\le \\theta$ </p>\n<p>通常不限制偏移b，$\\theta$ 越小，意味着更强的正则项。通过这种硬性限制，来调整w的大小，防止w过大</p>\n<p>使用均方范数作为柔性条件：</p>\n<p>对于每一个$\\theta$ 我们都可以找到一个$\\lambda$ 使得之前的目标函数等价于下式。</p>\n<p>$min \\ l(w, b)+\\frac{\\lambda}{2}||w||^2$ </p>\n<p>超参数$\\lambda$ 控制了正则项的重要程度，如果为0，则没作用，如果$\\lambda -&gt; \\infty$ 则w* -&gt; 0 </p>\n<p><img src=\"/../photos/18.png\" alt=\"img\"></p>\n<p>由公式中可以看出，通过引入$\\lambda$ 我们能够人为的控制$w_t$的大小。</p>\n<h2 id=\"丢弃法-dropout\"><a href=\"#丢弃法-dropout\" class=\"headerlink\" title=\"丢弃法(dropout)\"></a>丢弃法(dropout)</h2><p>丢弃法：就是在层之间加入噪音。但是呢，我们希望加入的噪音，不会影响数据的结果，最起码是平均结果。也就是说$E[x’] &#x3D; x $其中，x’为x加入噪音之后的值。 </p>\n<p>丢弃法对每个元素进行如下扰动：</p>\n<p>$x_i’ &#x3D; \\begin{cases} 0 \\ with \\ probablity \\ p \\ \\frac{x_i}{1-p} \\ otherwise \\end{cases}$</p>\n<p>丢弃法使用在隐藏全连接层的输出上，假设隐藏层输出为h，那么下一层的输入将会变成$h’ &#x3D; dropout(h)$。从结果上来看，在隐藏层上添加了dropout之后，相当于随机的丢弃了隐藏层中的一些节点，减少了模型规模。训练中dropout如上使用，在推理中，不需要dropout 的操作，所以此时的dropout输入就等于输出。</p>\n<p><strong>这个丢弃概率p是一个超参数</strong></p>\n<h2 id=\"数值稳定性\"><a href=\"#数值稳定性\" class=\"headerlink\" title=\"数值稳定性\"></a>数值稳定性</h2><p>数值稳定性问题，本质上是因为模型的深度增加之后，因为梯度的累乘，导致梯度爆炸或者消失的问题。最终导致训练结果不好。</p>\n<p>解决办法：为了让梯度更稳定，也就是在合理的范围内，可以尝试让乘法变成加法，或者将梯度归一化，梯度裁剪等。</p>\n<p><strong>最重要的还是合理的权重初始化和激活函数。</strong></p>\n<p>Xavier初始化：使得$\\gamma_t(n_{t-1}+n_t)&#x2F;2 &#x3D; 1 \\rightarrow \\ \\gamma_t &#x3D; \\frac{2}{n_{t-1}+n_t}$ </p>\n<ol>\n<li>正态分布：$N(0, \\sqrt{\\frac{2}{n_{t-1}+n_t}})$ </li>\n<li>均匀分布：$u(-\\sqrt{\\frac{6}{n_{t-1}+n_t}},\\sqrt{\\frac{6}{n_{t-1}+n_t}})$ ——均匀分布[-a, a]和方差是$\\frac{a^2}{3}$</li>\n</ol>\n<h1 id=\"Chapter-5-Deep-Learning-Calculation\"><a href=\"#Chapter-5-Deep-Learning-Calculation\" class=\"headerlink\" title=\"Chapter 5 Deep Learning Calculation\"></a>Chapter 5 Deep Learning Calculation</h1><h2 id=\"模型构造\"><a href=\"#模型构造\" class=\"headerlink\" title=\"模型构造\"></a>模型构造</h2><p>使用继承自nn.module的类，来自己构造一个模型，或者说是nn中的一个块，需要重写init和forward函数，同时也可以自己重写一个sequence函数，从而实现nn套nn的情况。</p>\n<h2 id=\"参数管理\"><a href=\"#参数管理\" class=\"headerlink\" title=\"参数管理\"></a>参数管理</h2><p>主要是nnmodule中的层中的参数都是可以随机访问，并且修改的。同时也支持apply按层初始化方法，不同层之间也可以apply不同的初始化方法。整体上nn框架的自由度还是很高的。</p>\n<h2 id=\"读写文件\"><a href=\"#读写文件\" class=\"headerlink\" title=\"读写文件\"></a>读写文件</h2><p>保存和加载张量：使用torch.save and torch.load</p>\n<p>加载参数列表的时候，不止要带过来params，同时模型也要一并带过来。</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># save the params </span>\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>net<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"XXX.params\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Load the params</span>\nclone <span class=\"token operator\">=</span> xxxModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 实例化对应的模型</span>\nclone<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">\"XXX.params\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nclone<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 这一步是将模型切换到评估模式，确保之后进行test的时候，确实是在测试，而不是在训练。</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h2 id=\"如何在GPU上运行NN\"><a href=\"#如何在GPU上运行NN\" class=\"headerlink\" title=\"如何在GPU上运行NN\"></a>如何在GPU上运行NN</h2><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># 获取gpu个数</span>\ntorch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 获取第i个GPU，0-base</span>\ngpu_name <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:i'</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 查看张量所在的设备上</span>\nx <span class=\"token operator\">=</span> tensor\nx<span class=\"token punctuation\">.</span>device <span class=\"token comment\"># 就是设备名称</span>\n\n<span class=\"token comment\"># 创建张量的时候可以声明在gpu上</span>\nx <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> device <span class=\"token operator\">=</span> gpu_name<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># 将张量移动到另外一块GPU上</span>\nX <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> device <span class=\"token operator\">=</span> gpu1<span class=\"token punctuation\">)</span>\nY <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> device <span class=\"token operator\">=</span> gpu2<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># 想让X和Y进行计算，需要将两个张量置于同一块GPU上，通过以下来移动</span>\nZ <span class=\"token operator\">=</span> X<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span> <span class=\"token comment\"># 将X复制一份，然后copy到第i块GPU上</span>\n\n\n<span class=\"token comment\"># 对于nn</span>\nnet<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device <span class=\"token operator\">=</span> gpu<span class=\"token punctuation\">)</span> 通过net<span class=\"token punctuation\">.</span>to方法来将整个网络移动到某块GPU上\n<span class=\"token comment\"># 使用data.device可以确认整个net都在同一块GPU上</span>\nnet<span class=\"token punctuation\">.</span>weight<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>device<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h1 id=\"Chapter-6-convolutional-neural-network，CNN\"><a href=\"#Chapter-6-convolutional-neural-network，CNN\" class=\"headerlink\" title=\"Chapter 6 convolutional neural network，CNN\"></a>Chapter 6 convolutional neural network，CNN</h1><h2 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h2><p>重新考察全连接层，将输入和输出变形成一个矩阵，将权重变成一个4-D张量，(h’, w’)</p>\n<p>两个原则：平移不变形，局部性。</p>\n<p>总结：对全连接层使用以上两个原则就得到了卷积层。</p>\n<p>所以说，卷积层又称为特殊的全连接层</p>\n<h2 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h2><h2 id=\"二维卷积层\"><a href=\"#二维卷积层\" class=\"headerlink\" title=\"二维卷积层\"></a>二维卷积层</h2><p>$Y &#x3D; X※W + b$</p>\n<p>其中※表示卷积操作，输入X：$n_h<em>n_w$ 核W：$k_h</em>k_w$, $b\\in R$ ，输出Y ：$(n_h-k_h+1) * (n_w-k_w+1)$ </p>\n<p>W和b都是可以学习的参数。</p>\n<p>所以，卷积层其实就是输入和卷积核进行交叉相关，加上bias得到输出。卷积核的大小是超参数。</p>\n<h2 id=\"填充和步幅\"><a href=\"#填充和步幅\" class=\"headerlink\" title=\"填充和步幅\"></a>填充和步幅</h2><p>由上式子不难发现，卷积之后维度都变小了，多进行几次维度都快没了。</p>\n<p>所以我们需要<strong>填充</strong>，对输入周围一圈填充一些数据。从而输出的大小和输入一样，甚至更大</p>\n<p><strong>步幅</strong>：指行&#x2F;列的滑动步长，默认为1，也就是一步一步移动过去。</p>\n<p>这两个也是个超参数</p>\n<h2 id=\"多输入多输出通道\"><a href=\"#多输入多输出通道\" class=\"headerlink\" title=\"多输入多输出通道\"></a>多输入多输出通道</h2><p>对于多输入通道，每一个输入都会有一个卷积核，分别做卷积操作，最后结果将各通道的结果加起来。</p>\n<p>对于多输出通道，每一个输出都会有一个对应的卷积核，分别做卷积操作，各通道卷积之后的结果就是该通道的输出结果。</p>\n<p><img src=\"/../photos/4.png\" alt=\"4\"></p>\n<p>对于多输入，多输出的情况，见上图演示。</p>\n<h2 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h2><p>池化层主要是为了降低卷积层对位置的敏感性。池化操作类似卷积，只不过卷积的过程改成了另外一些算法</p>\n<p>常见池化层算法：1. 取max， 2. 取平均值</p>\n<p>池化层和卷积层一样，可以调kernel的大小，调整padding，stride等等超参数。</p>\n<h2 id=\"LeNET\"><a href=\"#LeNET\" class=\"headerlink\" title=\"LeNET\"></a>LeNET</h2><p><img src=\"/../photos/5.png\" alt=\"5\"></p>\n<h1 id=\"chapter-7-modern-CNN\"><a href=\"#chapter-7-modern-CNN\" class=\"headerlink\" title=\"chapter 7 modern CNN\"></a>chapter 7 modern CNN</h1><p>AlexNet , VGG , NiN , GoogLeNet </p>\n<h2 id=\"批量归一化-batch-normalization\"><a href=\"#批量归一化-batch-normalization\" class=\"headerlink\" title=\"批量归一化(batch normalization)\"></a>批量归一化(batch normalization)</h2><p>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放。可以加快收敛速度，一般不改变模型精度</p>\n<p>具体归一化方法如下：</p>\n<p><img src=\"/../photos/6.png\" alt=\"6\"></p>\n<p>可以学习的参数就是$\\gamma$和$\\beta$ 。可以作用在全连接层和卷积层的输出上，激活函数前，也可以作用在他们的输入上。</p>\n<p>对全连接层，作用在特征维度。</p>\n<p>对卷积层，作用在通道维。</p>\n<p>最初，批量归一化认为可以减少内部协变量转移，反而是通过在小批量中添加了噪音来控制模型复杂度。噪音就是每个batch数据的均值和方差。So， 没必要和dropout一起用。</p>\n<h2 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h2><p>残差网络的产生源自于一个问题：加更多的层总是能改进精度吗？</p>\n<p>当添加过多的层的时候，可能导致模型偏移最优解的地方。但是如果不同层直接nested，那就可以避免这个问题。</p>\n<p>resnet中，添加了一个fx &#x3D; x + gx的思想，也就是一块的输入可以跳过这块，直接到他的输出那边，然后相加，作为最终的结果输出。如图：</p>\n<p><img src=\"/../photos/7.png\" alt=\"7\"></p>\n<p>以上是两种resnet块的方法，区别就在于走捷径的那边有没有一个1*1的卷积。当然resnet块也有很多其他的version，不同version的效果也都不一样。还有两种resnet块，高宽减半resnet块(strider &#x3D; 2)和高宽不变的。</p>\n<p>总体的架构和VGG挺像的。</p>\n<p>残差块使得很深的网络更加容易训练，因为更小的网络被nest进去了，训练起来被nest进去的会优先训练好，然后再逐渐往外训练。这个捷径的使用，其实有点像动态规划。</p>\n<h2 id=\"densenet\"><a href=\"#densenet\" class=\"headerlink\" title=\"densenet\"></a>densenet</h2><p>densenet总体来说和resnet差不多，只不过加操作变成了concat操作，导致了输出通道数的增加，同时，他concat的是一个稠密块中所有前面的结果，所以densenet必须还要带一个过渡层，从而抑制模型复杂度的增加。</p>\n<h1 id=\"Chapter-8-RNN\"><a href=\"#Chapter-8-RNN\" class=\"headerlink\" title=\"Chapter 8 RNN\"></a>Chapter 8 RNN</h1><h2 id=\"文本预处理\"><a href=\"#文本预处理\" class=\"headerlink\" title=\"文本预处理\"></a>文本预处理</h2><p> 按行读入整段文本，简单预处理一下一些字符（比如大写改小写，非字母字符改成空格）$\\rightarrow$ tokenize化，将文本行拆成word或者char $\\rightarrow$ 根据tokenize之后的结果，构建词汇表(vocabulary)。</p>\n<h2 id=\"语言模型\"><a href=\"#语言模型\" class=\"headerlink\" title=\"语言模型\"></a>语言模型</h2><p>给定文本序列x1,x2…xT, 语言模型的目的是估计联合概率p(x1,x2,…..xT)。</p>\n<h2 id=\"N元语法\"><a href=\"#N元语法\" class=\"headerlink\" title=\"N元语法\"></a>N元语法</h2><p>对于一个长度为m的序列，n为总词数，n()表示里面特定词或者词对出现的次数。</p>\n<p>$p(x_1, x_2, x_3…..x_m)&#x3D;p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)…..p(x_m|x_1,x_2,…..x_{m-1})$</p>\n<p>$&#x3D;\\frac{n(x_1)}{n}\\frac{n(x_1,x_2)}{n(x_1)}\\frac{n(x_1,x_2,x_3)}{n(x_1,x_2)}….&#x3D;\\frac{n(x_1,x_2,x_3,….x_m)}{n}$</p>\n<p>如果一个序列比较长，因为文本量不够大，所以分子上可能会小于等于1。这个时候可以使用马尔科夫假设来缓解这个问题。</p>\n<p>所谓N元语法实际上就是马尔科夫假设中采用了N-1的长度。</p>\n<p>比如我采用了i的长度，那么我们预测这个值我们只看前i个，如果i&#x3D;0，则各序列保持独立。相应的，p求解公式也会变化。</p>\n<h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><p>RNN使用了一个隐变量h来表示过去的信息。$h_t由h_{t-1}和x_{t-1}决定，而当前输出o_t 由h_t决定$</p>\n<p>lossfuntion：RNN本质上其实还是个分类问题，预测下一个字符是哪个，选择概率最大的即可。所以loss当然可以选择交叉熵。这里RNN引入了一个新的lossfunction——困惑度（实际上就是对交叉熵作一个e(x)的运算）</p>\n<p>梯度裁剪：迭代中计算到这T个时间步上的维度，在backward的时候会有一个长O（T）的矩阵乘法链，导致数值不稳定。而梯度裁剪可以有效预防梯度爆炸。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>g 表示所有层的梯度</p>\n<p>$g &#x3D; min(1, \\frac{\\theta}{||g||}) * g$</p>\n<p>说白了就是当g的层数超过$\\theta$ 的时候，通过min运算，将g的层数控制在$\\theta$ 以内。</p></blockquote>\n<h1 id=\"Chapter-9-Modern-RNN\"><a href=\"#Chapter-9-Modern-RNN\" class=\"headerlink\" title=\"Chapter 9 Modern RNN\"></a>Chapter 9 Modern RNN</h1><h2 id=\"GRU\"><a href=\"#GRU\" class=\"headerlink\" title=\"GRU\"></a>GRU</h2><p>GRU建立在一个问题上——序列模型中，有些数据是不重要的，可以忽略，有些数据是很重要的，需要着重学习。以此添加了一个门的概念。</p>\n<p>两种门：更新门$Z_t$和重置门$R_t$</p>\n<p><img src=\"/../photos/8.png\" alt=\"8\"></p>\n<p>由此可以控制Ht多学习一点Ht-1还是现在的Xt</p>\n<h2 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h2><p>LSTM和GRU差不多，只不过两种门改成了三种门：忘记门，输入门，输出门。然后进行相应的参数计算。本质上也还是将GRU的思想一样，通过调成过去状态的比重，来遗忘一些过去的信息，多记住一些现在的信息。</p>\n<h2 id=\"深度RNN\"><a href=\"#深度RNN\" class=\"headerlink\" title=\"深度RNN\"></a>深度RNN</h2><p>实际上就是增加隐藏层的数量，类似MLP的思想</p>\n<h2 id=\"双向RNN\"><a href=\"#双向RNN\" class=\"headerlink\" title=\"双向RNN\"></a>双向RNN</h2><p><img src=\"/../photos/9.png\" alt=\"9\"></p>\n<p>双向LSTM，非常不适合做推理，因为看不到未来的信息。但是很适合作序列抽取特征和填空。</p>\n<h2 id=\"编码器-解码器\"><a href=\"#编码器-解码器\" class=\"headerlink\" title=\"编码器-解码器\"></a>编码器-解码器</h2><p>编码器：将输入编程成中间形式（特征）。</p>\n<p>解码器：将中间形式转化为输出。</p>\n<p>由此可以得到编码器-解码器架构，一个模型就可以分为两块，编码器和解码器。</p>\n<h2 id=\"seq2seq\"><a href=\"#seq2seq\" class=\"headerlink\" title=\"seq2seq\"></a>seq2seq</h2><p>seq2seq是一个模型，用于从一个句子到另外一个句子，使用的是编码器解码器的架构。编码器和解码器里面使用的都是RNN，编码器中RNN最后一层的状态直接作为解码器的初始状态。</p>\n<p>使用BLEU来衡量生成结果的好坏。</p>\n<h2 id=\"束搜索-beam-search\"><a href=\"#束搜索-beam-search\" class=\"headerlink\" title=\"束搜索(beam search)\"></a>束搜索(beam search)</h2><p>在seq2seq中，我们预测每一个字符，都是选取的概率最大的那个。这是一种贪心的搜索。贪心得到的是局部最优解，往往不是全局最优解。当然我们可以暴搜，穷举全部的情况，当然这样时间复杂度太大，不现实。</p>\n<p>beam search本质上就是穷举加剪枝，不过他这个剪枝不是基于每个子树层面的，而是每一层层面的。具体表现为bs给定一个k，表示我们保留每一层前k大概率的预测结果，然后根据这k种可能再去预测下一个字符。因为每次保留都是保留每一层上前k个，而不是每一个子树上前k个。所以他的时间复杂度是O(nkT)——n是字典大小，因为字典中的每一个值都有一个概率。</p>\n<h1 id=\"Chapter-10-attention-mechanism\"><a href=\"#Chapter-10-attention-mechanism\" class=\"headerlink\" title=\"Chapter 10 attention mechanism\"></a>Chapter 10 attention mechanism</h1><h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>心理学认为人通过随意线索和不随意线索选择注意点。注意力机制添加了类似的功能。</p>\n<p>注意力机制显式的考虑随意线索，随意线索被称为查询(query)，非随意线索被称为keys，每一个输入都是一个(key, value)的键值对，通过注意力池化层来有偏好的选择某些输入。\t</p>\n<h3 id=\"非参注意力池化层\"><a href=\"#非参注意力池化层\" class=\"headerlink\" title=\"非参注意力池化层\"></a>非参注意力池化层</h3><p>就是池化层没参数的，如下图的公式</p>\n<p><img src=\"/../photos/10.png\" alt=\"10\"></p>\n<p>其中K()函数就是核函数，这里核函数使用高斯核$K(u)&#x3D;\\frac{e^{-\\frac{u^2}{2}}}{\\sqrt{2\\pi}}$ 带入K函数之后，可以得到以下结果。</p>\n<p><img src=\"/../photos/11.png\" alt=\"11\"></p>\n<p>中间那一步实际上就是做了一个softmax子操作，毕竟分母就是分子的各种情况的求和。</p>\n<h3 id=\"参数注意力池化层\"><a href=\"#参数注意力池化层\" class=\"headerlink\" title=\"参数注意力池化层\"></a>参数注意力池化层</h3><p>参数化的实际上就是引入了可以学习的w，在无参基础上，我们将fx修改成</p>\n<p>$f(x)&#x3D;\\sum_{i&#x3D;1}^{n} softmax(-\\frac{((x-x_i)*w)^2}{2})y_i$ 其中w就是可以学习的参数。</p>\n<p>实际上注意力机制的总体思想和GRU，LSTM是类似的，都是提取主要特征和内容，从而加强记忆用于之后的预测。</p>\n<p>查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚（就是那个池化层）。注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。</p>\n<h2 id=\"注意力分数\"><a href=\"#注意力分数\" class=\"headerlink\" title=\"注意力分数\"></a>注意力分数</h2><p><img src=\"/../photos/12.png\" alt=\"12\"></p>\n<p>注意力分数是query和keys的相似度，计算注意力分数有多种方法。而注意力权重就是将注意力分数做一次softmax的结果。如果拓展到多维，公式如下：</p>\n<p><img src=\"/../photos/13.png\" alt=\"13\"></p>\n<h3 id=\"Additive-Attention\"><a href=\"#Additive-Attention\" class=\"headerlink\" title=\"Additive Attention\"></a>Additive Attention</h3><p>增加了可学参数$W_k\\in R^{h<em>k}, W_q\\in R^{h</em>q}, v\\in R^h$</p>\n<p>此时的$a(k, q) &#x3D;v^T tanh(W_k k + W_q q)$ 实际上W*k和W*q 的过程是将k和q都化为长为h的向量，之后再和v转置做乘法得到一个值作为a的值。</p>\n<p>等价于将k和q合并之后，放入一个隐藏层大小为h，输出为1的单隐藏层MLP。</p>\n<h3 id=\"scaled-dot-product-attention\"><a href=\"#scaled-dot-product-attention\" class=\"headerlink\" title=\"scaled dot-product attention\"></a>scaled dot-product attention</h3><p>如果query和key都是相同的长度d，那么可以</p>\n<p>$a(q, k_i) &#x3D; \\frac{&lt;q, k_i&gt;}{\\sqrt{d}}$</p>\n<p>向量化版本如下：</p>\n<p><img src=\"/../photos/14.png\" alt=\"14\"></p>\n<h2 id=\"自注意力机制\"><a href=\"#自注意力机制\" class=\"headerlink\" title=\"自注意力机制\"></a>自注意力机制</h2><p>自注意力池化层将xi当做key，value，query来对序列抽取特征。</p>\n<h2 id=\"CNN-RNN-Self-Attention比较\"><a href=\"#CNN-RNN-Self-Attention比较\" class=\"headerlink\" title=\"CNN,RNN,Self-Attention比较\"></a>CNN,RNN,Self-Attention比较</h2><p><img src=\"/../photos/15.png\" alt=\"15\"></p>\n<h2 id=\"位置编码\"><a href=\"#位置编码\" class=\"headerlink\" title=\"位置编码\"></a>位置编码</h2><p>CNN和RNN中都能够体现位置信息，而自注意力中却不可以，所以需要引入位置编码在输入中加入位置信息，从而让自注意力能够记忆位置信息。</p>\n<h2 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h2><p>与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的ℎ组不同的 <em>线性投影</em>（linear projections）来变换查询、键和值。 然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这ℎ个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为<em>多头注意力</em>（multihead attention）。</p>\n<p>对于ℎ个注意力汇聚输出，每一个注意力汇聚都被称作一个<em>头</em>（head）</p>\n<p>也就是说，每一个注意力就是一个头，最终我们将这多个头的汇聚结果concat起来。期间都要做线性变换。</p>\n<h2 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h2><p>Transformer使用encoder-decoder架构，纯使用注意力机制，没有使用RNN，将一系列层的操作包装成一个transformer块。encoder和decoder都有n个块。</p>\n<h1 id=\"Chapter-11-Optimization\"><a href=\"#Chapter-11-Optimization\" class=\"headerlink\" title=\"Chapter 11 Optimization\"></a>Chapter 11 Optimization</h1><p>常见优化算法：梯度下降及其变种，动量法，adagrad，RMSprop， adadelta，adam。</p>\n<h1 id=\"Chapter-12-computational-performance\"><a href=\"#Chapter-12-computational-performance\" class=\"headerlink\" title=\"Chapter 12 computational-performance\"></a>Chapter 12 computational-performance</h1><p>GPU并行计算，异步处理，多GPU训练，多服务器多GPU分布式训练。</p>\n<h1 id=\"Chapter13-Computer-Vision\"><a href=\"#Chapter13-Computer-Vision\" class=\"headerlink\" title=\"Chapter13 Computer Vision\"></a>Chapter13 Computer Vision</h1><h2 id=\"图像增广\"><a href=\"#图像增广\" class=\"headerlink\" title=\"图像增广\"></a>图像增广</h2><p>其实就是对原图像做各种操作，比如随机切割，改颜色，饱和度，透明度等等，使得能够产生更多的不同的图像，增加数据集的量和不同之处。获取多样性从而使得模型泛化性能更好。</p>\n<h2 id=\"微调\"><a href=\"#微调\" class=\"headerlink\" title=\"微调\"></a>微调</h2><p>使用一个pre-train过的模型，来用自己的数据集来训练自己的模型。自己的模型参数初始化使用已经训练好的。</p>\n<p>源数据集（预训练模型使用的数据集）远远复杂于目标数据，通常微调的效果更好。</p>\n<h2 id=\"锚框\"><a href=\"#锚框\" class=\"headerlink\" title=\"锚框\"></a>锚框</h2><p>一类目标检测算法基于锚框来实现的。锚框其实也就是一个框，只不过他属于是检测框的预测。我们最后需要在锚框中选择一个作为检测框。</p>\n<p>首先先生成大量锚框，然后赋予标号，每个标况作为一个样本去训练。</p>\n<p>使用IoU来计算两个框之间的相似度。$IoU &#x3D; \\frac{相交的部分}{并起来的部分}$ 去除多余锚框的时候基于非极大值抑制（NMS），也就是选中非背景类预测值最大的，去掉所有和他IoU值超过一个阈值$\\theta$ 的，重复这个过程，直到所有预测要么被选中，要么被去掉。</p>\n<h2 id=\"R-CNNs\"><a href=\"#R-CNNs\" class=\"headerlink\" title=\"R-CNNs\"></a>R-CNNs</h2><p>R-CNN是一种regional-CNN 是卷积神经网络，同时是最早的使用锚框做目标检测的CNN。</p>\n<p>Fast R-CNN和Faster R-CNN在处理速度上进行了优化。Mask R-CNN则是在处理精度上进行了优化。</p>\n<h2 id=\"单发多框检测-SSD-single-shot-detection\"><a href=\"#单发多框检测-SSD-single-shot-detection\" class=\"headerlink\" title=\"单发多框检测(SSD, single shot detection)\"></a>单发多框检测(SSD, single shot detection)</h2><p>single shot 也就是看一次。不像R-CNN，有一个主CNN和一个辅CNN来处理同一个图像。SSD只有一个CNN处理。</p>\n<p>SSD会对每个像素，生成多个以他为中心的锚框，SSD以一个基础网络来抽取特征，然后多个卷积层块来减半高宽，在每段都会生成锚框，对锚框预测类别和边缘框。</p>\n<h2 id=\"语义分割\"><a href=\"#语义分割\" class=\"headerlink\" title=\"语义分割\"></a>语义分割</h2><p>语义分割是将图片中不同东西都描边描出来，比如猫猫狗狗都分出来，还有背景也分出来。涂成不同的颜色。</p>\n<p>语义分割是将图片中的每个像素分类到对应的类别。</p>\n<h2 id=\"转置卷积\"><a href=\"#转置卷积\" class=\"headerlink\" title=\"转置卷积\"></a>转置卷积</h2><p>转置卷积实际上是卷积的逆过程。</p>\n<p>卷积是将输入的宽高变小或者不变，而转置卷积则是将输入的宽高变大或者不变。</p>\n<p>转置卷积同样有padding和strike。和卷积相反，padding反而会让结果变小，strike反而会让结果更大。</p>\n<p>转置卷积转化为卷积的过程如下图：</p>\n<p><img src=\"/../photos/16.png\" alt=\"16\"></p>\n<p>形状换算如下图所示：</p>\n<p><img src=\"/../photos/17.png\" alt=\"17\"></p>\n<p>转置卷积不等于数学上的反卷积。两个不是一个东西。</p>\n<h2 id=\"全连接卷积神经网络FCN\"><a href=\"#全连接卷积神经网络FCN\" class=\"headerlink\" title=\"全连接卷积神经网络FCN\"></a>全连接卷积神经网络FCN</h2><p>FCN将CNN中最后两层全局池化层和全连接层，替换成了1*1的卷积层和一个转置卷积层，最后会得到一个扩大之后的图像，并且通道数就等于类别数，从而能够实现对每一个像素点的类别预测。</p>\n<h2 id=\"样式迁移\"><a href=\"#样式迁移\" class=\"headerlink\" title=\"样式迁移\"></a>样式迁移</h2><p>将样式图片中的样式迁移到内容图片上，得到合成图片。这就是样式迁移。</p>\n<h1 id=\"Chapter-14-NLP-pretrained\"><a href=\"#Chapter-14-NLP-pretrained\" class=\"headerlink\" title=\"Chapter 14 NLP-pretrained\"></a>Chapter 14 NLP-pretrained</h1><h2 id=\"word2vec\"><a href=\"#word2vec\" class=\"headerlink\" title=\"word2vec\"></a>word2vec</h2><h3 id=\"词嵌入\"><a href=\"#词嵌入\" class=\"headerlink\" title=\"词嵌入\"></a>词嵌入</h3><p>词向量是用于表示单词意义的向量。将单词映射到实向量的技术称为词嵌入。独热编码也是一种词嵌入，只不过这种方法效果很差。</p>\n<h3 id=\"word2vec-1\"><a href=\"#word2vec-1\" class=\"headerlink\" title=\"word2vec\"></a>word2vec</h3><p>word2vec工具是为了解决独热编码问题而产生的。它也将每个词映射到一个向量上，只不过这个向量可以更好的表达不同次之间的相似性和类比关系。word2vec包含两个模型，跳元模型和连续词袋。</p>\n<h4 id=\"跳元模型\"><a href=\"#跳元模型\" class=\"headerlink\" title=\"跳元模型\"></a>跳元模型</h4><p>考虑中心词预测上下文词。</p>\n<p>在两种模型中，每一个词都用两个d维向量表示，词典中索引为i的词有$v_i$ 和$u_i$ 表示中心词和上下文词，在词袋模型中他们的含义正好相反，要注意。</p>\n<h4 id=\"连续词袋模型\"><a href=\"#连续词袋模型\" class=\"headerlink\" title=\"连续词袋模型\"></a>连续词袋模型</h4><p>考虑上下文预测中心词。</p>\n<h2 id=\"近似训练\"><a href=\"#近似训练\" class=\"headerlink\" title=\"近似训练\"></a>近似训练</h2><p>上述两种模型中，都用到了softmax子操作来预测结果，而softmax子操作中会涉及整个词表大小一样多的项的求和，这就导致梯度计算非常的昂贵，为了解决这个问题出现了近似训练。</p>\n<p>一共有两种。</p>\n<ol>\n<li>负采样。负采样通过让采样中添加负样本而不仅是正样本，使得推理的时候不涉及整个词表大小数量的项的和，而仅仅涉及正负样本数量的项的和。大大减少了梯度的计算量</li>\n<li>层序softmax。层序softmax将词表构建成了一颗二叉树，分层softmax将原本的损失函数更改成了和词w，以及从根节点到词w的路径上的节点相关，而不是跟整个词表相关。从而大大减少了梯度计算。</li>\n</ol>\n<h2 id=\"GloVe\"><a href=\"#GloVe\" class=\"headerlink\" title=\"GloVe\"></a>GloVe</h2><p>GloVe是在word2vec上基础上改的，该模型结合了<strong>全局词频统计信息</strong>和<strong>局部上下文信息</strong> 。能够很好的利用高相关性的词对。GloVe主要对跳元模型做了三个修改，一个是使用$x_{ij}$ 和$e^{u_j^T v_i}$ 而不是概率分布，取对数之后计算平方损失。其次为每个词添加了中心词bias和上下文词bias，最后使用权重h(x)来替换每个损失项的权重，hx一方面可以省略掉$x_{ij}$ 为零的，也就是出现次数为零的项，另外一方面可以控制高频词的权重，防止高频词减缓训练速度。基于这三点修改，GLOVE重构了损失函数。</p>\n<h2 id=\"子词嵌入\"><a href=\"#子词嵌入\" class=\"headerlink\" title=\"子词嵌入\"></a>子词嵌入</h2><p>普通词嵌入没有考虑到词和变形词之间的相近性，同时也没有考虑诸如girl,girlfriend和boy,boyfriend的关系相近。也就是说没有对词的内部结构进行探讨。</p>\n<h3 id=\"fastText模型\"><a href=\"#fastText模型\" class=\"headerlink\" title=\"fastText模型\"></a>fastText模型</h3><p>fastText提出了子词嵌入的方法，其中子词是一个字符的n-gram。fastText可以认为是子词级的跳元模型，每个中心词由其子词向量之和表示。</p>\n<p>对于一个单词where，首先在他的首尾加上&lt;&gt;变成&lt;where&gt; 然后求他的n-gram，也就是所有的长度为n的子串——子词。假设n&#x3D;3，可以获得如下子词“&lt;wh”“whe”“her”“ere”“re&gt;”和特殊子词“&lt;where&gt;”。</p>\n<p>在fastText中，对于任意词w，用Gw表示其长度在3和6之间的所有子词与其特殊子词的并集。词表是所有词的子词的集合。假设$z_g$是词典中的子词g的向量，则跳元模型中作为中心词的词w的向量$v_w$是其子词向量的和：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>$v_w &#x3D; \\sum_{g \\in G_W} z_g$ </p></blockquote>\n<h3 id=\"字节对编码-BPE\"><a href=\"#字节对编码-BPE\" class=\"headerlink\" title=\"字节对编码(BPE)\"></a>字节对编码(BPE)</h3><p>BPE 的核心思想是通过逐步合并文本中最频繁的字符对，减少词汇量并创建更紧凑的表示方式。以下是 BPE 的主要步骤：</p>\n<ol>\n<li><strong>初始化字符集</strong>：将所有词分解成单个字符作为初始单位。</li>\n<li><strong>统计字符对频率</strong>：统计所有字符对（即相邻字符组合）的频率，找出出现次数最多的字符对。</li>\n<li><strong>合并字符对</strong>：将频率最高的字符对替换为一个新符号（将这个字符对视为一个整体）。</li>\n<li><strong>迭代合并</strong>：重复步骤 2 和 3，直到达到预定的合并次数或词汇表大小。</li>\n</ol>\n<p>因为合并的始终是频率最高的，所以其实是一种贪心算法。</p>\n<h1 id=\"Chapter-15-NLP-Application\"><a href=\"#Chapter-15-NLP-Application\" class=\"headerlink\" title=\"Chapter 15 NLP-Application\"></a>Chapter 15 NLP-Application</h1><p>NLP应用现在主要是微调预训练好的模型，对于很多应用任务，只需要在预训练好的模型后面添加一些层就可以完成任务了。</p>\n<p>NLP应用主要有：</p>\n<ol>\n<li><p>句子分类——将&lt;cls&gt;对应的向量输入到全连接层做分类问题</p>\n</li>\n<li><p>命名实体识别——识别一个单词是不是命名实体或者专有名词，比如人名，机构等。做法是将非特殊词元输入到全连接层做分类。</p>\n</li>\n<li><p>问题回答——给定一个问题和描述问题，找出一个片段作为回答。做法是对描述文字中的每一个词预测他是不是回答的开始或结束。</p>\n</li>\n<li><p>情感分析——研究人们在文本中的情感，这被认为是一个文本分类问题。</p>\n</li>\n<li><p>自然语言推断——<em>自然语言推断</em>（natural language inference）主要研究 <em>假设</em>（hypothesis）是否可以从<em>前提</em>（premise）中推断出来， 其中两者都是文本序列。 换言之，自然语言推断决定了一对文本序列之间的逻辑关系。这类关系通常分为三种类型：</p>\n<ul>\n<li><em>蕴涵</em>（entailment）：假设可以从前提中推断出来。</li>\n<li><em>矛盾</em>（contradiction）：假设的否定可以从前提中推断出来。</li>\n<li><em>中性</em>（neutral）：所有其他情况。</li>\n</ul>\n<p>实际上自然语言推断就是一个推理的过程，给定前提让模型判断是否能够推断出来假设。</p>\n</li>\n</ol>\n","feature":true,"text":"Chapter1&amp;2 Introduction&amp;Pre-Knowledge张量的概念——由数值组成的数组，可以有多个维度。 torch.arange(x)–生成一个0-x的一维张量，左开右闭。 reshape和shape和numpy中的一样。 torch.nume...","link":"","photos":[],"count_time":{"symbolsCount":"17k","symbolsTime":"16 mins."},"categories":[{"name":"AI","slug":"AI","count":2,"path":"api/categories/AI.json"}],"tags":[{"name":"AI","slug":"AI","count":2,"path":"api/tags/AI.json"},{"name":"DeepLearning","slug":"DeepLearning","count":1,"path":"api/tags/DeepLearning.json"},{"name":"DL","slug":"DL","count":1,"path":"api/tags/DL.json"},{"name":"Model","slug":"Model","count":1,"path":"api/tags/Model.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter1-amp-2-Introduction-amp-Pre-Knowledge\"><span class=\"toc-text\">Chapter1&amp;2  Introduction&amp;Pre-Knowledge</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">广播机制</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%95%B0%E6%8D%AE\"><span class=\"toc-text\">处理缺失的数据</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80\"><span class=\"toc-text\">数学基础</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%8C%83%E6%95%B0\"><span class=\"toc-text\">范数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5\"><span class=\"toc-text\">正交矩阵</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BD%AE%E6%8D%A2%E7%9F%A9%E9%98%B5\"><span class=\"toc-text\">置换矩阵</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF-Hadamard-product\"><span class=\"toc-text\">哈达玛积(Hadamard product)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#L-2-%E8%8C%83%E6%95%B0%E6%98%AF%E5%90%91%E9%87%8F%E5%85%83%E7%B4%A0%E5%B9%B3%E6%96%B9%E5%92%8C%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9\"><span class=\"toc-text\">$L_2$范数是向量元素平方和的平方根</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#L-1-%E8%8C%83%E6%95%B0%E6%98%AF%E5%90%91%E9%87%8F%E5%85%83%E7%B4%A0%E7%9A%84%E7%BB%9D%E5%AF%B9%E5%80%BC%E4%B9%8B%E5%92%8C\"><span class=\"toc-text\">$L_1$范数是向量元素的绝对值之和</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BA%9A%E5%AF%BC%E6%95%B0\"><span class=\"toc-text\">亚导数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E5%85%AC%E5%BC%8F\"><span class=\"toc-text\">矩阵求导公式</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC\"><span class=\"toc-text\">自动求导</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E5%AE%9E%E7%8E%B0\"><span class=\"toc-text\">自动求导实现</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-3-Linear-Neural-Network\"><span class=\"toc-text\">Chapter 3 Linear Neural Network</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Linear-regression\"><span class=\"toc-text\">Linear regression</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%B0%83%E7%94%A8%E7%8E%B0%E6%9C%89%E6%A1%86%E6%9E%B6%E6%9D%A5%E8%BF%9B%E8%A1%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BD%BF%E7%94%A8\"><span class=\"toc-text\">调用现有框架来进行线性回归的使用</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">损失函数</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#softmax%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">softmax回归</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#pytorch%E5%AE%9E%E7%8E%B0\"><span class=\"toc-text\">pytorch实现</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%A1%E7%AE%97%E5%87%86%E7%A1%AE%E7%8E%87%E5%92%8C%E5%87%86%E7%A1%AE%E7%9A%84%E4%B8%AA%E6%95%B0\"><span class=\"toc-text\">计算准确率和准确的个数</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-4-%E6%84%9F%E7%9F%A5%E6%9C%BA\"><span class=\"toc-text\">Chapter 4 感知机</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%84%9F%E7%9F%A5%E6%9C%BA\"><span class=\"toc-text\">感知机</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86\"><span class=\"toc-text\">感知机收敛定理</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7\"><span class=\"toc-text\">感知机的局限性</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-MLP\"><span class=\"toc-text\">多层感知机(MLP)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80-weight-decay\"><span class=\"toc-text\">权重衰退(weight decay)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%B8%A2%E5%BC%83%E6%B3%95-dropout\"><span class=\"toc-text\">丢弃法(dropout)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7\"><span class=\"toc-text\">数值稳定性</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-5-Deep-Learning-Calculation\"><span class=\"toc-text\">Chapter 5 Deep Learning Calculation</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0\"><span class=\"toc-text\">模型构造</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86\"><span class=\"toc-text\">参数管理</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">读写文件</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A6%82%E4%BD%95%E5%9C%A8GPU%E4%B8%8A%E8%BF%90%E8%A1%8CNN\"><span class=\"toc-text\">如何在GPU上运行NN</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-6-convolutional-neural-network%EF%BC%8CCNN\"><span class=\"toc-text\">Chapter 6 convolutional neural network，CNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF\"><span class=\"toc-text\">卷积</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF%E5%B1%82\"><span class=\"toc-text\">卷积层</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82\"><span class=\"toc-text\">二维卷积层</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85\"><span class=\"toc-text\">填充和步幅</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93\"><span class=\"toc-text\">多输入多输出通道</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B1%A0%E5%8C%96%E5%B1%82\"><span class=\"toc-text\">池化层</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#LeNET\"><span class=\"toc-text\">LeNET</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#chapter-7-modern-CNN\"><span class=\"toc-text\">chapter 7 modern CNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96-batch-normalization\"><span class=\"toc-text\">批量归一化(batch normalization)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#ResNet\"><span class=\"toc-text\">ResNet</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#densenet\"><span class=\"toc-text\">densenet</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-8-RNN\"><span class=\"toc-text\">Chapter 8 RNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86\"><span class=\"toc-text\">文本预处理</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">语言模型</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#N%E5%85%83%E8%AF%AD%E6%B3%95\"><span class=\"toc-text\">N元语法</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#RNN\"><span class=\"toc-text\">RNN</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-9-Modern-RNN\"><span class=\"toc-text\">Chapter 9 Modern RNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#GRU\"><span class=\"toc-text\">GRU</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#LSTM\"><span class=\"toc-text\">LSTM</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6RNN\"><span class=\"toc-text\">深度RNN</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8F%8C%E5%90%91RNN\"><span class=\"toc-text\">双向RNN</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8\"><span class=\"toc-text\">编码器-解码器</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#seq2seq\"><span class=\"toc-text\">seq2seq</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%9D%9F%E6%90%9C%E7%B4%A2-beam-search\"><span class=\"toc-text\">束搜索(beam search)</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-10-attention-mechanism\"><span class=\"toc-text\">Chapter 10 attention mechanism</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">注意力机制</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%9D%9E%E5%8F%82%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%A0%E5%8C%96%E5%B1%82\"><span class=\"toc-text\">非参注意力池化层</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%A0%E5%8C%96%E5%B1%82\"><span class=\"toc-text\">参数注意力池化层</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0\"><span class=\"toc-text\">注意力分数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Additive-Attention\"><span class=\"toc-text\">Additive Attention</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#scaled-dot-product-attention\"><span class=\"toc-text\">scaled dot-product attention</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">自注意力机制</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#CNN-RNN-Self-Attention%E6%AF%94%E8%BE%83\"><span class=\"toc-text\">CNN,RNN,Self-Attention比较</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81\"><span class=\"toc-text\">位置编码</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B\"><span class=\"toc-text\">多头注意力</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Transformer\"><span class=\"toc-text\">Transformer</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-11-Optimization\"><span class=\"toc-text\">Chapter 11 Optimization</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-12-computational-performance\"><span class=\"toc-text\">Chapter 12 computational-performance</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter13-Computer-Vision\"><span class=\"toc-text\">Chapter13 Computer Vision</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF\"><span class=\"toc-text\">图像增广</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%BE%AE%E8%B0%83\"><span class=\"toc-text\">微调</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%94%9A%E6%A1%86\"><span class=\"toc-text\">锚框</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#R-CNNs\"><span class=\"toc-text\">R-CNNs</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8B-SSD-single-shot-detection\"><span class=\"toc-text\">单发多框检测(SSD, single shot detection)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2\"><span class=\"toc-text\">语义分割</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF\"><span class=\"toc-text\">转置卷积</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFCN\"><span class=\"toc-text\">全连接卷积神经网络FCN</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB\"><span class=\"toc-text\">样式迁移</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-14-NLP-pretrained\"><span class=\"toc-text\">Chapter 14 NLP-pretrained</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#word2vec\"><span class=\"toc-text\">word2vec</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AF%8D%E5%B5%8C%E5%85%A5\"><span class=\"toc-text\">词嵌入</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#word2vec-1\"><span class=\"toc-text\">word2vec</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B7%B3%E5%85%83%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">跳元模型</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">连续词袋模型</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%BF%91%E4%BC%BC%E8%AE%AD%E7%BB%83\"><span class=\"toc-text\">近似训练</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#GloVe\"><span class=\"toc-text\">GloVe</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%AD%90%E8%AF%8D%E5%B5%8C%E5%85%A5\"><span class=\"toc-text\">子词嵌入</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#fastText%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">fastText模型</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81-BPE\"><span class=\"toc-text\">字节对编码(BPE)</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-15-NLP-Application\"><span class=\"toc-text\">Chapter 15 NLP-Application</span></a></li></ol>","author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"mapped":true,"prev_post":{},"next_post":{"title":"ACM学习笔记day34 (DP专题的总结)","uid":"2c328a2145590af1c8c2b559a8951062","slug":"ACM学习笔记day34","date":"2023-03-19T10:29:55.000Z","updated":"2023-04-08T10:53:59.571Z","comments":true,"path":"api/articles/ACM学习笔记day34.json","keywords":null,"cover":"/img/codeforces.jpg","text":"已经很久没有写笔记了。主要是我一方面我摆（bushi,另外一个就是，后面的各种DP其实写笔记也没啥好写的，主要是理解这个DP是啥，然后也没有对应的板子好写的。自然也就不更新笔记了。那么最近是把基础的DP都学完了，作一个总结，大致的讲一下DP到底是个啥。DP的分类有啥怎么分类的。 ...","link":"","photos":[],"count_time":{"symbolsCount":"2.3k","symbolsTime":"2 mins."},"categories":[{"name":"ACM","slug":"ACM","count":42,"path":"api/categories/ACM.json"}],"tags":[{"name":"ACM","slug":"ACM","count":39,"path":"api/tags/ACM.json"},{"name":"打卡","slug":"打卡","count":35,"path":"api/tags/打卡.json"},{"name":"笔记","slug":"笔记","count":3,"path":"api/tags/笔记.json"},{"name":"DP","slug":"DP","count":5,"path":"api/tags/DP.json"},{"name":"动态规划","slug":"动态规划","count":5,"path":"api/tags/动态规划.json"},{"name":"专题总结","slug":"专题总结","count":1,"path":"api/tags/专题总结.json"}],"author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"feature":true}}