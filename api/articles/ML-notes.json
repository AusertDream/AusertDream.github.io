{"title":"ML notes","uid":"615e3f2233f9034a4c7de4aa41e90c29","slug":"ML-notes","date":"2024-01-07T12:27:41.000Z","updated":"2024-05-20T02:19:18.630Z","comments":true,"path":"api/articles/ML-notes.json","keywords":null,"cover":"/img/machine_learning.jpg","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>ML教材为《机器学习_周志华》</p>\n<h1 id=\"第一章\"><a href=\"#第一章\" class=\"headerlink\" title=\"第一章\"></a>第一章</h1><h3 id=\"PAC理论模型\"><a href=\"#PAC理论模型\" class=\"headerlink\" title=\"PAC理论模型\"></a>PAC理论模型</h3><p>PAC(Probably Approximately Correct 概率近似正确) learning model由Leslie Valiant提出。</p>\n<p>$$<br>\\begin{aligned}<br>P(|f(x)-y|\\le \\epsilon )\\ge 1-\\delta<br>\\end{aligned}<br>$$</p>\n<h3 id=\"基本术语\"><a href=\"#基本术语\" class=\"headerlink\" title=\"基本术语\"></a>基本术语</h3><p>数据集，训练，测试</p>\n<p>示例(instance)，样例(example)——样例就是示例带上label标记，也就是结果的单条数据</p>\n<p>样本(sample)</p>\n<p>属性(attribute)，特征(feature)，属性值——有时候称属性为特征</p>\n<p>属性空间&#x3D;样本空间&#x3D;输入空间</p>\n<p>特征向量——在属性空间中的示例</p>\n<p>标记空间，输出空间</p>\n<p>假设(hypothesis)，真相(ground-truth)，学习器(learner)</p>\n<p>分类——输出结果是离散的，二分类，多分类区别于输出不同结果的数量（比如二分类就好或者坏，多分类则有大中小）。二分类中有术语，正类和反类，正反类不是绝对的，可以交换。</p>\n<p>回归——输出结果是连续的</p>\n<p>监督学习(supervised learning)——有导师的学习，有label的示例作为输入。</p>\n<p>无监督学习(unsupervised learning)——无导师的学习，无label的示例作为输入。也就是区别于数据集中是否已经告诉你示例的label结果。</p>\n<p>未见样本(unseen instance)——未来的数据输入，未知</p>\n<p>未知“分布”——数据背后的规律</p>\n<p>独立同分布(i.i.d)——指样本是由同一个分布中独立的取出来的</p>\n<p>泛化(generalization)——泛化能力指的是模型对新数据的处理能力，是从特殊到一般的过程，也有特化，是一般到特殊的过程。</p>\n<h3 id=\"归纳偏好\"><a href=\"#归纳偏好\" class=\"headerlink\" title=\"归纳偏好\"></a>归纳偏好</h3><p>表示机器学习算法在学习过程中对某种类型假设的偏好</p>\n<p><strong>任何一个有效的机器学习算法必有其偏好</strong></p>\n<p>判断的一般原则：奥卡姆剃刀(Occam‘s razor)——若非必要，勿增实体，也就是说对于多个模型都能解释一个现象，我们选择一个最简单的即可。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>学习算法的归纳偏好是否与问题本身相匹配，大多数时候直接决定了算法能否取得好的性能。</p></blockquote>\n<h3 id=\"NFL定理\"><a href=\"#NFL定理\" class=\"headerlink\" title=\"NFL定理\"></a>NFL定理</h3><p>NFL(No Free Lunch Theorem)定理：一个算法A在某些问题上比算法B好，那么一定存在另外有一些问题B比A好。</p>\n<p>NFL定理的重要前提：所有问题出现的机会相同，所有问题同样重要。然而实际上我们只关注自己正在解决的问题。</p>\n<h3 id=\"泛化能力\"><a href=\"#泛化能力\" class=\"headerlink\" title=\"泛化能力\"></a>泛化能力</h3><p>泛化误差：在“未来”的样本上的误差</p>\n<p>经验误差：在训练集上的误差，也叫训练误差</p>\n<p>过拟合(overfitting) vs 欠拟合(underfitting) 如下图所示：</p>\n<p><img src=\"/../img/ml/overfitAndUnderfit.png\" alt=\"overfitAndUnderfit\"></p>\n<p>上图表示出了，训练误差并不是越小越好。</p>\n<h3 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h3><p>根据三个问题：</p>\n<p><strong>如何获得测试结果？→   评估方法</strong></p>\n<p>关键：怎么获得”测试集”</p>\n<p>常见方法：</p>\n<p><strong>留出法(hold-out)</strong></p>\n<p>也就是将训练数据分一部分出来用来测试。</p>\n<p>需要注意：1.保持数据划分的一致性（分层采样）2.多次重复划分（100次随机划分）3.测试集不能太大也不能太小（例如：1&#x2F;5~1&#x2F;3）</p>\n<p><strong>交叉验证法(cross validation)</strong></p>\n<p>又称k-折(fold)交叉验证法。</p>\n<p>例如10折交叉验证，也就是说将数据集分成10份，首次用前9个训练，第十个用于测试，然后轮转，用第九个测试，其他训练，如此往复，进行十次，得到的十次结果作平均返回。</p>\n<p>为了减少切分的误差，所以分成10份的过程，也还要做10次，也就是要做100次。</p>\n<p>如果k&#x3D;m的话，也就是k等于数据量，则得到”留一法”(leave-one-out,LOO) → 可能会导致测试结果误差较大。\t</p>\n<p><img src=\"/../img/ml/cross_validation.png\" alt=\"cross_validation\"></p>\n<p><strong>自助法(bootstrap)</strong></p>\n<p>基于“自助采样”(bootstrap sampling)，也叫“有放回的采样”，“可重复采样”。也就是说将所有数据看做一个盲盒，每次采样的时候抽取一个，然后放回去，同时再复制一个这个数据。</p>\n<p>缺点：1.训练集和原样本集同规模 2.数据分布有所改变</p>\n<p> 算法的参数：一般由人工设定，也叫“超参数”</p>\n<p>模型的参数：一般由学习确定</p>\n<p>验证集：区别于测试集，主要是用于调整模型参数的。是训练集中的专门用来调参用的。</p>\n<p><strong>如何评估性能优劣？→   性能度量</strong></p>\n<p> 回归任务常用均方误差：</p>\n<p><img src=\"/../img/ml/1.png\" alt=\"1\"></p>\n<p>对于分类任务：</p>\n<p><img src=\"/../img/ml/2.png\" alt=\"2\"></p>\n<p><img src=\"/../img/ml/3.png\" alt=\"3\"></p>\n<p>查准率(precision)：也就是在所有模型找出来的正样例里面，有多少是查对的，确确实实是正样例的。</p>\n<p>查全率(recall)：在所有的正样例中，模型找出来了多少。</p>\n<p>P和R的综合使用：</p>\n<p><img src=\"/../img/ml/4.png\" alt=\"4\"></p>\n<p>F1度量其实就是P和R的调和平均数，如果对哪一方有一点偏好的话，假设偏好是beta，那么对应的公式就是下面框起来的那个。</p>\n<p>beta&gt;1的时候查全率有更大影响，beta&lt;1时查准率有更大的影响</p>\n<p><strong>如何判断实质差别？→   比较检验</strong></p>\n<p> 统计假设检验为学习器性能比较提供了重要依据。</p>\n<p>常用检验方法：1.交叉验证<a href=\"https://zhuanlan.zhihu.com/p/138711532\">t检验</a>（基于成对t检验）2.McNemar检验（基于列联表，卡方检验）</p>\n<h3 id=\"线性模型\"><a href=\"#线性模型\" class=\"headerlink\" title=\"线性模型\"></a>线性模型</h3><p>线性模型(linear model)试图学得一个通过属性的线性组合来进行预测的函数。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>f(x) &#x3D; w1x1+w2x2+w3x3+….+b</p>\n<p>向量形式 :  f(x) &#x3D; <strong>w</strong>^T<strong>x</strong> +b</p></blockquote>\n<p>xi是对应的属性，wi是对应的权重。</p>\n<h4 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h4><p>线性回归(Linear Regression)是要求得以下内容</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>f(xi)&#x3D;wxi+b 使得 f(xi) ≈yi</p></blockquote>\n<p>计算以上内容的关键在于将属性数值化，而对于离散的属性不好数值化，采用如下方法：若有序（order），则可以连续化；否则，转化为k维向量。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>比如，西瓜有青色的这个属性，是离散的属性，不管青色还是蓝色还是绿色，没有明显的序关系，那么就将这个属性置入向量中用0和1来表示。[010]总左到右分别表示是否是青色，蓝色，绿色，其中一个是1，就表示是的。</p></blockquote>\n<p>对于线性回归想要学到的这个表达式f(xi)&#x3D;wxi+b，如何确定w和b，其实就是衡量f(x)和y的差别，均方误差是回归任务中常用的性能度量，因此我们可以让均方误差最小化，从而确定w和b。(arg min 就是使后面这个式子达到最小值时的变量的取值)</p>\n<p><img src=\"/../img/ml/5.png\" alt=\"5\"></p>\n<p>基于均方误差最小化来进行模型求解的方法称为“最小二乘法”，在线性回归中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧氏距离之和最小。</p>\n<p>求解w和b使得上图中右式值最小化的过程，称为线性回归模型的最小二乘“参数估计”。事实上，此时就是一个数学问题，对于一个二元函数f(w,b)，让你求f函数的最小值，问你w和b的值。</p>\n<h4 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h4><p>对于多元情况，其实就是如下情况：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>f(xi)&#x3D;<strong>w</strong>T<strong>x</strong>i+b 使得 f(<strong>x</strong>i) ≈yi</p>\n<p>其中<strong>x</strong>i&#x3D;(xi1;xi2;xi3….;xid)   yi属于R</p>\n<p>我们可以将b看作b*1，从而将右式合并为:<strong>w</strong>‘ * <strong>X</strong></p>\n<p>其中w’&#x3D;(<strong>w</strong>;b)，X&#x3D;(<strong>x</strong>i;<strong>1</strong>)，这个这个1是一个全是1的列向量，和xi对齐的。</p></blockquote>\n<p>类似的，我们使用最小二乘法来求w和b，也就是w’。</p>\n<p><img src=\"/../img/ml/6.png\" alt=\"6\"></p>\n<p>令偏导等于0求w’即可，需要用到矩阵求逆。</p>\n<p>如果X^T*X满秩或者正定，则<img src=\"/../img/ml/7.png\" alt=\"7\"></p>\n<p>如果不满秩，则可以解出来多个w’。此时需要求助于归纳偏好，或者引入正则化（regularization）</p>\n<h4 id=\"广义线性模型\"><a href=\"#广义线性模型\" class=\"headerlink\" title=\"广义线性模型\"></a>广义线性模型</h4><p>广义线性模型相较于线性模型，其实就是用表达式去逼近y的衍生物，比如逼近lny等等，一般形式如下:</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>y &#x3D; g^(-1) (w^T*x + b)</p></blockquote>\n<p>这个g^(-1)称为单调可微的联系函数(link function)。</p>\n<p>形式上来看，其实就是通过这个联系函数来实现逼近y的衍生物。</p>\n<p>广义线性模型的例子：</p>\n<p><strong>对率回归</strong></p>\n<p>对于实值输出是连续值，但期望输出是离散值的情况，我们需要一个联系函数，从而使用线性回归模型来解决这类问题。常用的函数如下，他单调可微，任意阶可导，被称为对数几率函数，简称“对率函数”</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>y&#x3D;1&#x2F;(1+e^(-z))</p></blockquote>\n<p>对上式进行带入变换，得到</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>ln(y&#x2F;(1-y)) &#x3D; w^T*x + b</p></blockquote>\n<p>其中y&#x2F;(1-y)称为几率（odds），反应了x作为正例的可能性。前面加了一个对数，称为“对数几率”(log odds,也叫logit)。对应的回归问题也相应的叫对率回归问题（logistic regression）。这个是分类学习算法。</p>\n<h4 id=\"线性判别分析\"><a href=\"#线性判别分析\" class=\"headerlink\" title=\"线性判别分析\"></a>线性判别分析</h4><p>线性判别分析(Linear discriminant analysis)，简称LDA。</p>\n<h1 id=\"基本机器学习算法及其实现\"><a href=\"#基本机器学习算法及其实现\" class=\"headerlink\" title=\"基本机器学习算法及其实现\"></a>基本机器学习算法及其实现</h1><h2 id=\"线性回归-1\"><a href=\"#线性回归-1\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2>","text":"前言ML教材为《机器学习_周志华》 第一章PAC理论模型PAC(Probably Approximately Correct 概率近似正确) learning model由Leslie Valiant提出。 $$\\begin{aligned}P(|f(x)-y|\\le \\epsi...","link":"","photos":[],"count_time":{"symbolsCount":"3.6k","symbolsTime":"3 mins."},"categories":[{"name":"AI","slug":"AI","count":2,"path":"api/categories/AI.json"}],"tags":[{"name":"AI","slug":"AI","count":2,"path":"api/tags/AI.json"},{"name":"Python","slug":"Python","count":2,"path":"api/tags/Python.json"},{"name":"ML","slug":"ML","count":1,"path":"api/tags/ML.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%89%8D%E8%A8%80\"><span class=\"toc-text\">前言</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC%E4%B8%80%E7%AB%A0\"><span class=\"toc-text\">第一章</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#PAC%E7%90%86%E8%AE%BA%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">PAC理论模型</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD\"><span class=\"toc-text\">基本术语</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%BD%92%E7%BA%B3%E5%81%8F%E5%A5%BD\"><span class=\"toc-text\">归纳偏好</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#NFL%E5%AE%9A%E7%90%86\"><span class=\"toc-text\">NFL定理</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B\"><span class=\"toc-text\">泛化能力</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">模型选择</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">线性模型</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">线性回归</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">多元线性回归</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">广义线性模型</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90\"><span class=\"toc-text\">线性判别分析</span></a></li></ol></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E6%9C%AC%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0\"><span class=\"toc-text\">基本机器学习算法及其实现</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1\"><span class=\"toc-text\">线性回归</span></a></li></ol></li></ol>","author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"mapped":true,"prev_post":{"title":"2024年终总结","uid":"ce7691ff8f2ef9994a806e6cb0977c21","slug":"2024年终总结","date":"2024-12-29T15:29:06.724Z","updated":"2024-12-29T15:29:06.726Z","comments":true,"path":"api/articles/2024年终总结.json","keywords":null,"cover":"/img/Ausert.jpg","text":"一年的时间真的过的很快，转眼间2024年已经行将尾声。现在再回首看向2024年的自己，感觉2024年的自己上下起伏还是很大的。也是自高考失败，插班生失败之后，第一次做成功了一件事。或许在之后，拼尽全力但是考插失败的阴影能够逐渐散去吧。 这一年间，我都干了点啥呢？保研结算，进到同济...","link":"","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[{"name":"年终总结","slug":"年终总结","count":1,"path":"api/categories/年终总结.json"}],"tags":[{"name":"年终总结","slug":"年终总结","count":1,"path":"api/tags/年终总结.json"},{"name":"碎碎念","slug":"碎碎念","count":1,"path":"api/tags/碎碎念.json"}],"author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"feature":false},"next_post":{"title":"Java Notes","uid":"f0b9b0e433bb738921b4a846a4505450","slug":"Java-Notes","date":"2024-01-07T12:21:49.000Z","updated":"2024-03-18T13:21:08.089Z","comments":true,"path":"api/articles/Java-Notes.json","keywords":null,"cover":"/img/javalogo.png","text":"序言因课设需要以及工作需要，不得不得开始学java了。 以下为作为学习java的整体笔记，包括一切有关java的架构或者已经写好的包。 java版本采用2021年之后的java17。 因为本身已经学过C&#x2F;C++了，所以对于java基础的内容仅简单的介绍和记笔记，不作过多...","link":"","photos":[],"count_time":{"symbolsCount":"15k","symbolsTime":"13 mins."},"categories":[{"name":"Java","slug":"Java","count":1,"path":"api/categories/Java.json"}],"tags":[{"name":"Java","slug":"Java","count":1,"path":"api/tags/Java.json"},{"name":"language","slug":"language","count":1,"path":"api/tags/language.json"},{"name":"Android Development","slug":"Android-Development","count":1,"path":"api/tags/Android-Development.json"}],"author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}}}}