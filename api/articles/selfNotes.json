{"title":"selfNotes","uid":"e51c6b0cc288c9b94c0f6d165fb0d5e1","slug":"selfNotes","date":"2025-03-24T03:58:23.871Z","updated":"2025-03-24T03:58:23.874Z","comments":true,"path":"api/articles/selfNotes.json","keywords":null,"cover":"https://zh.d2l.ai/_images/front.png","content":"<p>以此note介绍LLM有关的trick或者一些信息差的东西。</p>\n<ol>\n<li><p>计算loss的时候记得把padding token屏蔽掉，不然模型学的全是padding token，根本不会输出其他token</p>\n</li>\n<li><p>LLM的学习率往往很低，不要以为0.0005就已经很小了，如果发现模型收敛到一定的值就不收敛了，试试看减小学习率，让他多训练一会（暴怒）。</p>\n</li>\n<li><p>LLM的prompt很重要，问题描述不清楚，往往模型的输出没办法align到你想要的输出。</p>\n</li>\n<li><p>数据集记得分一个dev dataset出来，用来方便开发，不然跑一次train dataset跑半天突然来个报错就老实了</p>\n</li>\n<li><p>多卡训练模型保存之后，state_dict上会多加一个module标记，这个时候如果想单卡推理，记得module删了，不然dict会对不上。</p>\n</li>\n<li><p>transformers库的模型，在生成的时候有generate和chat两个方法，generate是单轮的，chat是多轮对话，注意chat_template。得严格按照模型的template来chat，不然很容易答非所问。此外虽然说LLM是可以多轮对话的，但实际上仍然是线性的，只不过是把history一股脑的给LLM而已，predict next token到结束。查看模型的template可以print(tokenizer.chat_template)查看，不过一般花里胡哨的，瞪眼法看也看不出来啥（</p>\n</li>\n<li><p>如果发现没有chat方法，那么如何方便的对齐chat_template呢？有两种方法：</p>\n<p>使用transformers库的<a href=\"https://huggingface.co/docs/transformers/v4.35.1/zh/pipeline_tutorial\">pipeline</a>:</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">pipe &#x3D; pipeline(\n    &quot;text-generation&quot;,\n    model&#x3D;modelConfig[&quot;model_name&quot;],\n    tokenizer&#x3D;tokenizer,\n    device_map&#x3D;&quot;cuda:3&quot;,\n)\noutputs &#x3D; pipe(\n    messages,\n    generation_config&#x3D;generation_config,\n)\nres &#x3D; outputs[0][&quot;generated_text&quot;][-1]\nreturn res</code></pre>\n\n<p>使用tokenizer的apply_chat_template，然后将结果tokenize之后，丢给generate方法去生成结果。</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"># 定义对话历史\nconversation &#x3D; [\n    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;,\n    &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;您好！有什么可以帮您？&quot;&#125;,\n    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;推荐一部科幻电影&quot;&#125;\n]\n\n# 转换为模型需要的格式\nformatted_text &#x3D; tokenizer.apply_chat_template(\n    conversation,\n    tokenize&#x3D;False,          # 返回字符串而不是 tokens\n    add_generation_prompt&#x3D;True  # 添加模型回复引导符\n)\n\ninputs &#x3D; tokenizer(formatted_text, return_tensors&#x3D;&quot;pt&quot;).to(&quot;cuda:3&quot;)\n\n# 生成回复\noutputs &#x3D; model.generate(\n    inputs.input_ids,\n    max_new_tokens&#x3D;500,\n    temperature&#x3D;0.7,\n    top_p&#x3D;0.9\n)\nprint(tokenizer.decode(outputs[0], skip_special_tokens&#x3D;True))</code></pre>\n\n<p>推荐使用pipeline，因为pipeline处理完之后返回的也是对话的列表，方便获取内容，这也是为什么上面代码中return的是[-1]的原因，因为返回最后一条，肯定是AI回答的。</p>\n</li>\n<li><p>partial方法：使用该方法可以冻结函数中部分参数，产生一个新的函数。</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">mt_image_loader &#x3D; partial(load_image, image_dir&#x3D;mt_images_dir)</code></pre>\n\n<p>如上代码中所示，load_image里面是包含两个参数的，一个是sample，一个是image_dir，这里使用partial相当于提前将image_dir这个参数给这个函数了，然后现在mt_image_loader是一个只有一个参数sample的函数了。可以直接给map方法用。</p>\n</li>\n</ol>\n","feature":false,"text":"以此note介绍LLM有关的trick或者一些信息差的东西。 计算loss的时候记得把padding token屏蔽掉，不然模型学的全是padding token，根本不会输出其他token LLM的学习率往往很低，不要以为0.0005就已经很小了，如果发现模型收敛到一定的值就不...","link":"","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[{"name":"LLM","slug":"LLM","count":1,"path":"api/categories/LLM.json"}],"tags":[{"name":"学习","slug":"学习","count":43,"path":"api/tags/学习.json"},{"name":"笔记本","slug":"笔记本","count":2,"path":"api/tags/笔记本.json"},{"name":"零碎的","slug":"零碎的","count":2,"path":"api/tags/零碎的.json"},{"name":"知识点","slug":"知识点","count":2,"path":"api/tags/知识点.json"},{"name":"深度学习","slug":"深度学习","count":1,"path":"api/tags/深度学习.json"},{"name":"大模型","slug":"大模型","count":1,"path":"api/tags/大模型.json"},{"name":"训练","slug":"训练","count":1,"path":"api/tags/训练.json"}],"toc":"","author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"mapped":true,"prev_post":{"title":"研究生基本功","uid":"5a49538b9703a5d95e8ed166fb3d78bc","slug":"研究生基本功","date":"2025-04-22T15:25:39.000Z","updated":"2025-04-22T15:30:56.877Z","comments":true,"path":"api/articles/研究生基本功.json","keywords":null,"cover":"/img/baseskills.jpg","text":"Linux简单使用 在进行以下操作之前，请先自行检索环境变量的作用，以及如何配置环境变量。 新用户装机步骤# 升级系统 sudo apt update # 更新软件包列表，切记不要upgrade # 配置 ssh，docker容器可以忽略 sudo apt install ope...","link":"","photos":[],"count_time":{"symbolsCount":"10k","symbolsTime":"9 mins."},"categories":[{"name":"基本功","slug":"基本功","count":1,"path":"api/categories/基本功.json"}],"tags":[{"name":"基本功","slug":"基本功","count":1,"path":"api/tags/基本功.json"},{"name":"必须会的东西","slug":"必须会的东西","count":1,"path":"api/tags/必须会的东西.json"},{"name":"不会可以remake了","slug":"不会可以remake了","count":1,"path":"api/tags/不会可以remake了.json"},{"name":"倒也不必","slug":"倒也不必","count":1,"path":"api/tags/倒也不必.json"},{"name":"研究生","slug":"研究生","count":1,"path":"api/tags/研究生.json"}],"author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}}},"next_post":{"title":"2024年终总结","uid":"ce7691ff8f2ef9994a806e6cb0977c21","slug":"2024年终总结","date":"2024-12-29T15:29:06.724Z","updated":"2024-12-29T15:29:06.726Z","comments":true,"path":"api/articles/2024年终总结.json","keywords":null,"cover":"/img/Ausert.jpg","text":"一年的时间真的过的很快，转眼间2024年已经行将尾声。现在再回首看向2024年的自己，感觉2024年的自己上下起伏还是很大的。也是自高考失败，插班生失败之后，第一次做成功了一件事。或许在之后，拼尽全力但是考插失败的阴影能够逐渐散去吧。 这一年间，我都干了点啥呢？保研结算，进到同济...","link":"","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[{"name":"年终总结","slug":"年终总结","count":1,"path":"api/categories/年终总结.json"}],"tags":[{"name":"年终总结","slug":"年终总结","count":1,"path":"api/tags/年终总结.json"},{"name":"碎碎念","slug":"碎碎念","count":1,"path":"api/tags/碎碎念.json"}],"author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"feature":false}}