{"name":"LLM","slug":"LLM","count":1,"postlist":[{"title":"myNotes","uid":"0d02e73f334fe4d979a797d2e188c2e3","slug":"myNotes","date":"2025-05-05T16:16:29.118Z","updated":"2025-05-05T16:16:31.828Z","comments":true,"path":"api/articles/myNotes.json","keywords":null,"cover":"https://zh.d2l.ai/_images/front.png","text":"以此note介绍和LLM有关，或者其他一些乱七八糟的东西。 计算loss的时候记得把padding token屏蔽掉，不然模型学的全是padding token，根本不会输出其他token LLM的学习率往往很低，不要以为0.0005就已经很小了，如果发现模型收敛到一定的值就不收...","link":"","photos":[],"count_time":{"symbolsCount":"2.4k","symbolsTime":"2 mins."},"categories":[{"name":"LLM","slug":"LLM","count":1,"path":"api/categories/LLM.json"}],"tags":[{"name":"学习","slug":"学习","count":43,"path":"api/tags/学习.json"},{"name":"笔记本","slug":"笔记本","count":2,"path":"api/tags/笔记本.json"},{"name":"零碎的","slug":"零碎的","count":2,"path":"api/tags/零碎的.json"},{"name":"知识点","slug":"知识点","count":2,"path":"api/tags/知识点.json"},{"name":"深度学习","slug":"深度学习","count":1,"path":"api/tags/深度学习.json"},{"name":"大模型","slug":"大模型","count":1,"path":"api/tags/大模型.json"},{"name":"训练","slug":"训练","count":1,"path":"api/tags/训练.json"}],"author":{"name":"Ausert","slug":"blog-author","avatar":"/img/Ausert.jpg","link":"/","description":"tech otakus save the world","socials":{"github":"https://github.com/AusertDream","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/102368527?spm_id_from=333.1007.0.0"},"codeforce":{"icon":"/svg/codeforces.svg","link":"https://codeforces.com/profile/Ausert"}}}},"feature":false}]}